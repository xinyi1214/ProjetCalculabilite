{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Suji04/NormalizedNerd/blob/master/Word2Vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##        Ce fichier va générer les données attendues par Spacy pour entrainement.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_N.B.: il s'agit ici du cas pour les ingrédients uniquement_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v2goQ7zZ6Jky"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_json('./Data/jsonCorpusRecettes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titre</th>\n",
       "      <th>preparation</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Poulet tandoori rouge</td>\n",
       "      <td>A préparer la veille !Mélanger l'épice Tandoor...</td>\n",
       "      <td>[4 escalopes de poulet, 4 cuillères à soupe d'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lasagnes végétariennes (facile)</td>\n",
       "      <td>Si vous utilisez des oignons, faites-les reven...</td>\n",
       "      <td>[6 tomates fraîches (ou pelées en boîte, à déf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Terrine de foie de porc</td>\n",
       "      <td>- hacher (pas trop fin) le foie, le lard, les ...</td>\n",
       "      <td>[400 grammes de foie de porc , 200 grammes de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lapin au vin blanc (à la cocotte)</td>\n",
       "      <td>Faire revenir les échalotes et les lardons dan...</td>\n",
       "      <td>[1,8 kg de lapin (7 à 8 morceaux), 1 bouteille...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Escalopes lucullus à la cocotte</td>\n",
       "      <td>Coupez l'oignon en petits morceaux, faites rev...</td>\n",
       "      <td>[4 escalopes, 4 tranches de bacon, 4 tranches ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               titre  \\\n",
       "0              Poulet tandoori rouge   \n",
       "1    Lasagnes végétariennes (facile)   \n",
       "2            Terrine de foie de porc   \n",
       "3  Lapin au vin blanc (à la cocotte)   \n",
       "4    Escalopes lucullus à la cocotte   \n",
       "\n",
       "                                         preparation  \\\n",
       "0  A préparer la veille !Mélanger l'épice Tandoor...   \n",
       "1  Si vous utilisez des oignons, faites-les reven...   \n",
       "2  - hacher (pas trop fin) le foie, le lard, les ...   \n",
       "3  Faire revenir les échalotes et les lardons dan...   \n",
       "4  Coupez l'oignon en petits morceaux, faites rev...   \n",
       "\n",
       "                                         ingredients  \n",
       "0  [4 escalopes de poulet, 4 cuillères à soupe d'...  \n",
       "1  [6 tomates fraîches (ou pelées en boîte, à déf...  \n",
       "2  [400 grammes de foie de porc , 200 grammes de ...  \n",
       "3  [1,8 kg de lapin (7 à 8 morceaux), 1 bouteille...  \n",
       "4  [4 escalopes, 4 tranches de bacon, 4 tranches ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23071, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.copy()\n",
    "# Format du Dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Chaque instruction (préparation) se compose de plusieurs phrases: on découpe en phrases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp=[]\n",
    "for i in range(df.shape[0]):    \n",
    "    tmp = tmp + re.split(\"[\" +'!?.'+ \"]+\" , df.preparation[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpTitre=[]\n",
    "for i in range(df.shape[0]):    \n",
    "    tmpTitre = tmpTitre + re.split(\"[\" +'!?.'+ \"]+\" , df.titre[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247190, 23226)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp),len(tmpTitre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> On constate que toutes les recettes n'ont pas de titre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence le prétraitement: en décollant et séparant les ponctuations dans un premier temps (pour la prépération mais aussi les titres) et en effaçant la casse (on passe tout les textes en minuscules)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "listPhrases=[]\n",
    "for i in range(len(tmp)):  \n",
    "    if tmp[i] != '':\n",
    "        listPhrases.append(tmp[i].replace('.',' . ').replace('!',' ! ').replace('?',' ? ').replace('\\'','  ').replace('\\n','').replace('(',' ( ').replace(')',' ) ').replace('\"',' ').lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "listPhrasesTitres=[]\n",
    "for i in range(len(tmpTitre)):  \n",
    "    if tmpTitre[i] != '':\n",
    "        listPhrasesTitres.append(tmpTitre[i].replace('.',' . ').replace('!',' ! ').replace('?',' ? ').replace('\\'','  ').replace('\\n','').replace('(',' ( ').replace(')',' ) ').replace('\"',' ').lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode qui prend en entrée un fichier de stopwords, qui le charge et renvoie la liste des stopwords\n",
    "def readJsonStopWordsInList(fileFrenchStopWords):\n",
    "    dataDict={}\n",
    "    with open(fileFrenchStopWords,encoding=\"utf8\") as json_data:\n",
    "        dataDict = json.load(json_data)\n",
    "    return dataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation, tokenisation, suppression des données non alphabetiques\n",
    "def initialNormalizeAndTokenize(phrase_test):\n",
    "\n",
    "    word_text =[]\n",
    "    word_text = nltk.word_tokenize(phrase_test)\n",
    "\n",
    "    # on peut enlever les ponctuations ce n est pas une analyse de sentiments.. \n",
    "    ponc =\"!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~»«…\\ '’\"\n",
    "    table = str.maketrans(\"\",\"\", ponc)\n",
    "\n",
    "    # convert to lower case\n",
    "    stripped = [w.translate(table).lower() for w in word_text]   \n",
    "        \n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    stripped = [word for word in stripped if word.isalpha()]\n",
    "    \n",
    "    assembled =\" \".join(stripped)\n",
    "    assembled = assembled.rstrip()\n",
    "    word_text = nltk.word_tokenize(assembled)    \n",
    "    return word_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatiseur (stemmer) en français\n",
    "def initialLemmatizer(word_text):\n",
    "\n",
    "    # lemmatizer = FrenchLefffLemmatizer()\n",
    "    # lemWords = [lemmatizer.lemmatize(word) for word in word_text]\n",
    "    stemmer = FrenchStemmer()\n",
    "    lemWords = [stemmer.stem(word) for word in word_text]\n",
    "\n",
    "    return lemWords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode qui va prétraiter en utilisant le radical (stemmer) en particulier\n",
    "def preprocessingUsingNorm(stopWords,text,adjWords):\n",
    "\n",
    "    textNorm=[]\n",
    "    for i,val in enumerate(text):\n",
    "        phrase_test=val     \n",
    "\n",
    "        # tokenization, suppression ponctuation et normalisation\n",
    "        word_text = initialNormalizeAndTokenize(phrase_test)\n",
    "    \n",
    "        # stopWords supprimes et qq adjectifs et adverbes\n",
    "        words = initialBow(stopWords,word_text,adjWords)\n",
    "        \n",
    "        \n",
    "        # II) Lemmatisation (il s'agit de Stemmer ici mais c'est équivalent)\n",
    "        lemWords = initialLemmatizer(words) \n",
    "        \n",
    "        #stripped = unique(words)\n",
    "        assembled =\" \".join(lemWords)\n",
    "        assembled = assembled.rstrip()\n",
    "        textNorm.append(assembled)   \n",
    "        \n",
    "    return textNorm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Méthode qui va supprimer les stopwords mais aussi une liste de mots (adjectifs, adverbes..)\n",
    "def initialBow(stopWords,word_text,adjWords):\n",
    "    tmp = [w for w in word_text if not w in stopWords]\n",
    "    words = [w for w in tmp if not w in adjWords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileFrenchStopWords = \"./Data/stopwords-fr.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on charge les stopWords\n",
    "\n",
    "stopWords={}\n",
    "stopWords = readJsonStopWordsInList(fileFrenchStopWords) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste de token à enlever de la liste également: ajectifs, adverbes...\n",
    "\n",
    "df_Adj =  pd.read_csv('./Data/adjectifCulinaire.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On met en lowercase\n",
    "for i in range(df_Adj.shape[0]):    \n",
    "    df_Adj.Liste_A_Enlever[i] = df_Adj['Liste_A_Enlever'][i].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Liste_A_Enlever</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>succulent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>savoureux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aromatique</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>allégé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>goûteux</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Liste_A_Enlever\n",
       "0       succulent\n",
       "1       savoureux\n",
       "2      aromatique\n",
       "3          allégé\n",
       "4         goûteux"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Adj.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjWords =  df_Adj['Liste_A_Enlever'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On normalise , on enlève les stopwords, des ajectifs et adverbes, les nombres, on lemmatise (stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "textNorm = preprocessingUsingNorm(stopWords,listPhrases,adjWords)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230494"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textNorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "listPhrasesTitresNorm = preprocessingUsingNorm(stopWords,listPhrasesTitres,adjWords)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des données de sorties de la clusturisation: une liste d'ingrédients (radical)\n",
    "\n",
    "df_ingredientsSave =  pd.read_csv('./Data/listIngredientsToSave.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IngredientsSave</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>limoncello</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>épinard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>combav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rapadur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mikados</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  IngredientsSave\n",
       "0      limoncello\n",
       "1         épinard\n",
       "2          combav\n",
       "3         rapadur\n",
       "4         mikados"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ingredientsSave.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1127, 1)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ingredientsSave.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation en liste\n",
    "ingredients = df_ingredientsSave['IngredientsSave'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy : EntityRuler et le label 'INGREDIENT' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Utilisation de _EntityRuler_ pour créer l'entité _'INGREDIENT'_ et faire le lien avec notre liste d'ingrédients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.pipeline import EntityRuler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.entityruler.EntityRuler at 0x23193a06b88>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "@Language.factory('ruler')\n",
    "def Ruler(nlp, name):\n",
    "\n",
    "    ingredient_pattern = [\n",
    "        {\"LOWER\": {\"IN\": ingredients}}\n",
    "    ]\n",
    "\n",
    "    patterns = [{\"label\": \"INGREDIENT\", \"pattern\": ingredient_pattern}]\n",
    "    return EntityRuler(nlp, patterns=patterns)\n",
    "\n",
    "nlp.add_pipe('ruler', before=\"ner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230494"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(textNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée nos data 80% train et 20% test: 184360 vs 46134\n",
    "corpus = textNorm[0:184359]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mélang épic tandoor épic indien roug lait yaourt grec huil jus citron', {'entities': [[7, 11, 'INGREDIENT'], [12, 19, 'INGREDIENT'], [20, 24, 'INGREDIENT'], [25, 31, 'INGREDIENT'], [37, 41, 'INGREDIENT'], [42, 48, 'INGREDIENT'], [49, 53, 'INGREDIENT'], [54, 58, 'INGREDIENT'], [59, 62, 'INGREDIENT'], [63, 69, 'INGREDIENT']]}]\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA = []\n",
    "\n",
    "#iterate over the corpus again and create spacy 2 data input (will be convert later)\n",
    "for sentence in corpus:\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    #remember, entities needs to be a dictionary in index 1 of the list, so it needs to be an empty list\n",
    "    entities = []\n",
    "    \n",
    "    #extract entities\n",
    "    for ent in doc.ents:\n",
    "\n",
    "        #appending to entities in the correct format\n",
    "        entities.append([ent.start_char, ent.end_char, ent.label_])\n",
    "        \n",
    "    if len(entities) > 0:\n",
    "        TRAIN_DATA.append([sentence, {\"entities\": entities}])\n",
    "        \n",
    "\n",
    "\n",
    "print (TRAIN_DATA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après traitement des données, la construction du data set donne:  144443  données pour le dataTrain \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Après traitement des données, la construction du data set donne: ',len(TRAIN_DATA),' données pour le dataTrain \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée nos data 80% train et 20% test: 184360 vs 46134\n",
    "corpusTest = textNorm[184360:230493]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mettr cuir moul savarin minut', {'entities': [[11, 15, 'INGREDIENT']]}]\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA = []\n",
    "\n",
    "#iterate over the corpus again\n",
    "for sentence in corpusTest:\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    #remember, entities needs to be a dictionary in index 1 of the list, so it needs to be an empty list\n",
    "    entities = []\n",
    "    \n",
    "    #extract entities\n",
    "    for ent in doc.ents:\n",
    "\n",
    "        #appending to entities in the correct format\n",
    "        entities.append([ent.start_char, ent.end_char, ent.label_])\n",
    "        \n",
    "    if len(entities) > 0:\n",
    "        TEST_DATA.append([sentence, {\"entities\": entities}])\n",
    "        \n",
    "\n",
    "print (TEST_DATA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après traitement des données, la construction du data set donne:  35568  données pour le dataTest \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Après traitement des données, la construction du data set donne: ',len(TEST_DATA),' données pour le dataTest \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Médthode pour sauvegarde de fichier json\n",
    "def save_data(file,data):\n",
    "    with open(file,\"w\",encoding='utf-8') as f:\n",
    "        json.dump(data,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23126"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpusTitresTest = listPhrasesTitresNorm[0:23225]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poulet tandoor roug', {'entities': [[0, 6, 'INGREDIENT'], [7, 14, 'INGREDIENT']]}]\n"
     ]
    }
   ],
   "source": [
    "TEST_TITRE_DATA = []\n",
    "\n",
    "#iterate over the corpus again\n",
    "for sentence in corpusTitresTest:\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    #remember, entities needs to be a dictionary in index 1 of the list, so it needs to be an empty list\n",
    "    entities = []\n",
    "    \n",
    "    #extract entities\n",
    "    for ent in doc.ents:\n",
    "\n",
    "        #appending to entities in the correct format\n",
    "        entities.append([ent.start_char, ent.end_char, ent.label_])\n",
    "        \n",
    "    if len(entities) > 0:\n",
    "        TEST_TITRE_DATA.append([sentence, {\"entities\": entities}])\n",
    "        \n",
    "    #TRAIN_DATA.append([sentence, {\"entities\": entities}])\n",
    "\n",
    "print (TEST_TITRE_DATA[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Après traitement des données, la construction du data set donne:  21648  données pour le data d'évaluation sur les titres\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Après traitement des données, la construction du data set donne: ',len(TEST_TITRE_DATA),' données pour le data d\\'évaluation sur les titres\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des données pour entrainement: train/test/eval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"./outputData/ingredients_training_data2.json\",TRAIN_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"./outputData/ingredients_test_data2.json\",TEST_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(\"./outputData/ingredientsTitre_test_data2.json\",TEST_TITRE_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des données pour la partie préparation/instruction des recettes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>prépar veil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mélang épic tandoor épic indien roug lait yaou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>découp escalop poulet petit morceau enlev méla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>repos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fair grill morceau poêl rôt brochet serv sauc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230489</th>\n",
       "      <td>ajout crem maizen oignon ciboulet sal poivr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230490</th>\n",
       "      <td>répart carr émiet fond tart versezy prépar oeuf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230491</th>\n",
       "      <td>dispos tomat appui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230492</th>\n",
       "      <td>enfourn min</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230493</th>\n",
       "      <td>serv salad assaison</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230494 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Instructions\n",
       "0                                             prépar veil\n",
       "1       mélang épic tandoor épic indien roug lait yaou...\n",
       "2       découp escalop poulet petit morceau enlev méla...\n",
       "3                                                   repos\n",
       "4       fair grill morceau poêl rôt brochet serv sauc ...\n",
       "...                                                   ...\n",
       "230489        ajout crem maizen oignon ciboulet sal poivr\n",
       "230490    répart carr émiet fond tart versezy prépar oeuf\n",
       "230491                                 dispos tomat appui\n",
       "230492                                        enfourn min\n",
       "230493                                serv salad assaison\n",
       "\n",
       "[230494 rows x 1 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Prep = pd.DataFrame({'Instructions': textNorm})\n",
    "df_Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On enregistre les données prétraitées pour la partie sur la section préparation une ligne correspond à une phrase\n",
    "df_Prep.to_csv('./Data/recettesParPhrases2.txt',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9qfbMxS1qDa"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Word2Vec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
